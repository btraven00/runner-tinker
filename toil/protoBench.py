import datetime
import itertools
import glob
import json
import time

from attrs import define, asdict
import pandas as pd

from toil.common import Toil
from toil.job import Job

# local imports
import params

# We are specifying global benchmark parameters as a dictionary of paramters to a list of integers, for simplicity.
# The idea is that the orchestrator will compute the combinatorial grid and schedule method runs for each combination
# of Data x ParameterValue.

# A more flexible parameter spec can of course be defined.
# This makes me think that we don't actually *need* a module for generating the parameters?
BENCHMARK_PARAMETERS = {
    'param1': [1, 2, 3],
    'param2': [10, 100, 1000]
}

# Pass equal resources to each runner by default. This likely needs some adaptation depending on the cluster backend used?
DEFAULT_RESOURCES = {'cores': 2, 'memory': '250M', 'disk':'100M'}

# This controls the parallelism for the singleMachine implementation. An equivalent semaphore will be needed
# when launching this in a docker/k8s cluster.
NUM_MAX_CORES=4

# Default duration of the dummy load (used by the executor)
RUN_FOR_SECONDS=2

def cpu_dummy_load(duration_sec=60):
    """
    A dummy load to simulate a CPU intensive task and be able to debug parallel execution. Nothing to see here.
    """
    start_time = time.time()
    end_time = start_time + duration_sec

    while time.time() < end_time:
        result = 0
        for i in range(1, 10000):
            result += i

@define
class JobExecutionMetadata:
    """
    Metadata that we want to capture for each JobExecution. This can be easily be serialized into the Knowledge Graph.
    """
    start: str = ""
    end: str = ""
    cores: int = 0
    disk: str = ""
    memory: str = ""

    parameters: dict = {}

# The worker function is used to retrieve instance data generated by the toil run.
WORKER_FN = 'worker'

# Suffix for the worker metadata file.
METADATA_SUFFIX = '-meta.json'

def worker(job, parameters, memory='1G', cores=2, disk='1G'):
    """
    Worker defines our main entrypoint to perform a computation (a benchmark method).
    We receive the job object, and a structured instance of `ParameterValue`.
    """
    t = job.fileStore.getLocalTempFile(suffix=METADATA_SUFFIX)

    meta = JobExecutionMetadata()
    # TODO: this is too simplistic - we're just capturing the CPU/MEM resources *assigned* by the Toil framework.
    # It would be good to extend this to capture real-world usage metrics.
    meta.cores = job.cores
    meta.disk = job.disk
    meta.memory = job.memory
    meta.start = str(datetime.datetime.now())
    meta.parameters = parameters

    # do some work!
    cpu_dummy_load(RUN_FOR_SECONDS)

    meta.end = str(datetime.datetime.now())

    # ... ideally, we would collect any useful results after this boilerplate.
    # Either numerical results serialized as json (for simple metrics), or perhaps a path for the global filesystem storage.

    # We finally save the job metadata in a globally accessible file in the fileStore provided by our
    # toil worker.
    # When all workers are done, the result collector will combine all 
    with open(t, 'w') as fH:
        fH.write(json.dumps(asdict(meta)))

    job.fileStore.writeGlobalFile(t)
    return f"executed with parameters: {parameters}"


def run_workflow():
    """
    Main entrypoint to run the (proto-)benchmark workflow.
    This could be run by the Orchestrator.

    In reality, we have a lot of flexibility here to construct a "CompositePlan" dynamically.

    We can use a generic state machine, but there's still some room to
    experiment with making the workflow reproducible (i.e., hook into renku's
    primitives).
    """
    parser = Job.Runner.getDefaultArgumentParser()
    options = parser.parse_args()
    options.maxCores = NUM_MAX_CORES

    # useful for debugging
    options.clean = "never"

    # generate the combinatorial grid of parameters
    parameters = params.parameterFactory(**BENCHMARK_PARAMETERS)

    with Toil(options) as toil:
        jobs = [Job.wrapJobFn(worker, asdict(parameterValue), **DEFAULT_RESOURCES) for parameterValue in list(parameters.grid())]

        root = Job()
        for job in jobs:
            root.addChild(job)

        output = toil.start(root)

    # we return the jobStore that we have just used, 
    # because the later stages (collector etc) will want to retrieve the temporary results.
    return options.jobStore

def collect_execution_metadata(fileStore):
    """
    A naive implementation of the metadata result collector. it receives the name for the toil fileStore
    (We're assuming a disk store for simplicity here).

    At this point, no matter what backend we used for Toil, we have access to an implementation 
    """

    # To collect files, we descend on all the per-job metadata files for all workers
    METADATA_GLOB = f"./{fileStore}/files/for-job/kind-{WORKER_FN}/instance-*/*/*{METADATA_SUFFIX}"
    meta_files = glob.glob(METADATA_GLOB)

    # this DataFrame is a naive result/metric collector, but I think it illustrates what I'm trying to explore here.
    # We can either collect numerical figures directly (if it's cheap enough),
    # or gather de-referenciable pointers to objects in a data storage solution
    # for further processing.

    # As an example, we're just gathering and displaying metadata information for the parameter combinations.

    # We could make this more flexible by importing custom metrics/fields from
    # the orchestrator definition (the orchestrator yaml file).

    meta_runs = pd.DataFrame([json.load(open(f)) for f in meta_files])
    print("Worker execution summary")
    print(meta_runs)
    return meta_runs

if __name__ == "__main__":
    store = run_workflow()
    fileStore = store.split('file:')[1]
    collect_execution_metadata(fileStore)
